{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-qcVbHmpe8sV","outputId":"8b11c305-20ab-47a7-a53b-e243753cff79","executionInfo":{"status":"ok","timestamp":1652343569344,"user_tz":-480,"elapsed":33673,"user":{"displayName":"IPSAL Lab-HUST","userId":"14403031417329069761"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G1Y7bJ7xfHVk","outputId":"33e2764c-7311-4e41-9838-b2f072f1aeff","executionInfo":{"status":"ok","timestamp":1652343570426,"user_tz":-480,"elapsed":1085,"user":{"displayName":"IPSAL Lab-HUST","userId":"14403031417329069761"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1-lqIvGv8GnqHULF2dwocEGdE1OZTOoFv/deepfish/affinity_lcfcn\n"]}],"source":["%cd /content/drive/MyDrive/deepfish/affinity_lcfcn"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"oQ_N_ZaztZNl","executionInfo":{"status":"ok","timestamp":1652343570426,"user_tz":-480,"elapsed":7,"user":{"displayName":"IPSAL Lab-HUST","userId":"14403031417329069761"}}},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"S6m1RkAL6xUe"},"source":["## \n","\n","## Get Started by following these 5 steps\n","- Step 1: Download the code and the dataset\n","- Step 2: Install and Import Libraries\n","- Step 3: Define List of Experiments\n","- Step 4: Train and Validate\n","- Step 5: Visualize the Results\n"]},{"cell_type":"markdown","metadata":{"id":"tUY5_d4jYsS0"},"source":["### Step 1: Download the code and the dataset"]},{"cell_type":"markdown","metadata":{"id":"eUl3ZL9GLRVF"},"source":["Open pickle file"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"id":"is-3vANB5h0d","outputId":"ebc42116-263c-4189-fd77-55aec63c765b","executionInfo":{"status":"ok","timestamp":1652343570426,"user_tz":-480,"elapsed":7,"user":{"displayName":"IPSAL Lab-HUST","userId":"14403031417329069761"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\n\\nimport pickle\\n\\n\\nwith open('/content/drive/MyDrive/deepfish/affinity_lcfcn/results/score_list.pkl', 'rb') as f:\\n    data = pickle.load(f)\\n\\ndata\\ndata[-1]\\n\\ndata[:-5]\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}],"source":["'''\n","\n","import pickle\n","\n","\n","with open('/content/drive/MyDrive/deepfish/affinity_lcfcn/results/score_list.pkl', 'rb') as f:\n","    data = pickle.load(f)\n","\n","data\n","data[-1]\n","\n","data[:-5]\n","'''"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"evqY27o6yxZ9","executionInfo":{"status":"ok","timestamp":1652343570427,"user_tz":-480,"elapsed":6,"user":{"displayName":"IPSAL Lab-HUST","userId":"14403031417329069761"}}},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":4,"metadata":{"id":"1zJUEufVMxUn","executionInfo":{"status":"ok","timestamp":1652343570427,"user_tz":-480,"elapsed":6,"user":{"displayName":"IPSAL Lab-HUST","userId":"14403031417329069761"}}},"outputs":[],"source":["# data\n","# [{'epoch': 0, 'loss': 0.06266696354613528, 'val_score': 0.7656167},\n","#  {'epoch': 1, 'loss': 0.02043819550022309, 'val_score': 0.8138912},\n","#  {'epoch': 2, 'loss': 0.013802118694722839, 'val_score': 0.85750055},\n","#  {'epoch': 3, 'loss': 0.011701151351931373, 'val_score': 0.84985864},\n","#  {'epoch': 4, 'loss': 0.009522941175156275, 'val_score': 0.85595626},\n","#  {'epoch': 5, 'loss': 0.008852865098006651, 'val_score': 0.8655461},\n","#  {'epoch': 6, 'loss': 0.009248249776427613, 'val_score': 0.8377928},\n","#  {'epoch': 7, 'loss': 0.009413797722129413, 'val_score': 0.88040924},\n","#  {'epoch': 8, 'loss': 0.008143163446221586, 'val_score': 0.8814454},\n","#  {'epoch': 9, 'loss': 0.006816251551985932, 'val_score': 0.88535804}]"]},{"cell_type":"markdown","metadata":{"id":"3PWATo3OY3W7"},"source":["## Step 2: Install and Import Libraries"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"rtGd0X0w7WPW","executionInfo":{"status":"ok","timestamp":1652343570428,"user_tz":-480,"elapsed":6,"user":{"displayName":"IPSAL Lab-HUST","userId":"14403031417329069761"}}},"outputs":[],"source":["# !pip install --upgrade --quiet git+https://github.com/haven-ai/haven-ai\n","# !pip install pydicom\n","# !pip install kornia\n","\n","# import tqdm.notebook as tqdm\n","# import os\n","# import torch\n","# import numpy as np\n","# import warnings\n","# warnings.filterwarnings('ignore')\n","\n","# from torch.utils.data import DataLoader\n","\n","# from haven import haven_examples as he\n","# from haven import haven_wizard as hw\n","# from haven import haven_jupyter as hj\n","# from haven import haven_results as hr\n","# from haven import haven_utils as hu\n","\n","# from src import models\n","# from src import datasets\n","# from src import utils as ut\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CfUT8mdItpZD","outputId":"f719e2d3-de58-4c17-e3c3-0f7998676860","executionInfo":{"status":"ok","timestamp":1652343594614,"user_tz":-480,"elapsed":24192,"user":{"displayName":"IPSAL Lab-HUST","userId":"14403031417329069761"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting haven-ai\n","  Downloading haven_ai-0.6.7-py3-none-any.whl (92 kB)\n","\u001b[K     |████████████████████████████████| 92 kB 872 kB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=0.0 in /usr/local/lib/python3.7/dist-packages (from haven-ai) (4.64.0)\n","Requirement already satisfied: numpy>=0.0 in /usr/local/lib/python3.7/dist-packages (from haven-ai) (1.21.6)\n","Requirement already satisfied: pandas>=0.0 in /usr/local/lib/python3.7/dist-packages (from haven-ai) (1.3.5)\n","Requirement already satisfied: notebook>=4.0 in /usr/local/lib/python3.7/dist-packages (from haven-ai) (5.3.1)\n","Requirement already satisfied: scipy>=0.0 in /usr/local/lib/python3.7/dist-packages (from haven-ai) (1.4.1)\n","Requirement already satisfied: torchvision>=0.0 in /usr/local/lib/python3.7/dist-packages (from haven-ai) (0.12.0+cu113)\n","Requirement already satisfied: jupyter>=0.0 in /usr/local/lib/python3.7/dist-packages (from haven-ai) (1.0.0)\n","Requirement already satisfied: requests>=0.0 in /usr/local/lib/python3.7/dist-packages (from haven-ai) (2.23.0)\n","Requirement already satisfied: ipywidgets>=0.0 in /usr/local/lib/python3.7/dist-packages (from haven-ai) (7.7.0)\n","Collecting opencv-python-headless>=0.0\n","  Downloading opencv_python_headless-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (47.8 MB)\n","\u001b[K     |████████████████████████████████| 47.8 MB 1.1 MB/s \n","\u001b[?25hRequirement already satisfied: scikit-image>=0.0 in /usr/local/lib/python3.7/dist-packages (from haven-ai) (0.18.3)\n","Requirement already satisfied: matplotlib>=0.0 in /usr/local/lib/python3.7/dist-packages (from haven-ai) (3.2.2)\n","Requirement already satisfied: Pillow>=0.0 in /usr/local/lib/python3.7/dist-packages (from haven-ai) (7.1.2)\n","Requirement already satisfied: sklearn>=0.0 in /usr/local/lib/python3.7/dist-packages (from haven-ai) (0.0)\n","Requirement already satisfied: scikit-learn>=0.0 in /usr/local/lib/python3.7/dist-packages (from haven-ai) (1.0.2)\n","Requirement already satisfied: torch>=0.0 in /usr/local/lib/python3.7/dist-packages (from haven-ai) (1.11.0+cu113)\n","Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=0.0->haven-ai) (0.2.0)\n","Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=0.0->haven-ai) (5.5.0)\n","Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=0.0->haven-ai) (5.3.0)\n","Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=0.0->haven-ai) (5.1.1)\n","Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=0.0->haven-ai) (1.1.0)\n","Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=0.0->haven-ai) (3.6.0)\n","Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=0.0->haven-ai) (4.10.1)\n","Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=0.0->haven-ai) (5.3.5)\n","Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=0.0->haven-ai) (5.1.1)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=0.0->haven-ai) (1.0.18)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=0.0->haven-ai) (57.4.0)\n","Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=0.0->haven-ai) (4.8.0)\n","Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=0.0->haven-ai) (0.8.1)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=0.0->haven-ai) (2.6.1)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=0.0->haven-ai) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=0.0->haven-ai) (0.7.5)\n","Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter>=0.0->haven-ai) (5.3.0)\n","Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter>=0.0->haven-ai) (5.2.0)\n","Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter>=0.0->haven-ai) (5.6.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=0.0->haven-ai) (0.11.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=0.0->haven-ai) (2.8.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=0.0->haven-ai) (1.4.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=0.0->haven-ai) (3.0.8)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=0.0->haven-ai) (4.2.0)\n","Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=0.0->haven-ai) (4.3.3)\n","Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=0.0->haven-ai) (4.10.0)\n","Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=0.0->haven-ai) (2.15.3)\n","Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=0.0->haven-ai) (5.7.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=0.0->haven-ai) (4.11.3)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=0.0->haven-ai) (0.18.1)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=0.0->haven-ai) (21.4.0)\n","Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=0.0->haven-ai) (3.8.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.0->haven-ai) (2.11.3)\n","Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.0->haven-ai) (0.13.3)\n","Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.0->haven-ai) (1.8.0)\n","Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=0.0->haven-ai) (22.3.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.0->haven-ai) (2022.1)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipywidgets>=0.0->haven-ai) (1.15.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipywidgets>=0.0->haven-ai) (0.2.5)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=0.0->haven-ai) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=0.0->haven-ai) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=0.0->haven-ai) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=0.0->haven-ai) (2.10)\n","Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.0->haven-ai) (2.4.1)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.0->haven-ai) (2.6.3)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.0->haven-ai) (1.3.0)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.0->haven-ai) (2021.11.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.0->haven-ai) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.0->haven-ai) (1.1.0)\n","Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook>=4.0->haven-ai) (0.7.0)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook>=4.0->haven-ai) (2.0.1)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=0.0->haven-ai) (0.8.4)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=0.0->haven-ai) (5.0.0)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=0.0->haven-ai) (1.5.0)\n","Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=0.0->haven-ai) (0.6.0)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=0.0->haven-ai) (0.7.1)\n","Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=0.0->haven-ai) (0.4)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter>=0.0->haven-ai) (0.5.1)\n","Requirement already satisfied: qtpy>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter>=0.0->haven-ai) (2.1.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from qtpy>=2.0.1->qtconsole->jupyter>=0.0->haven-ai) (21.3)\n","Installing collected packages: opencv-python-headless, haven-ai\n","Successfully installed haven-ai-0.6.7 opencv-python-headless-4.5.5.64\n","Collecting kornia\n","  Downloading kornia-0.6.4-py2.py3-none-any.whl (493 kB)\n","\u001b[K     |████████████████████████████████| 493 kB 5.0 MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from kornia) (1.11.0+cu113)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from kornia) (21.3)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.8.1->kornia) (4.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->kornia) (3.0.8)\n","Installing collected packages: kornia\n","Successfully installed kornia-0.6.4\n","Found existing installation: opencv-python-headless 4.5.5.64\n","Uninstalling opencv-python-headless-4.5.5.64:\n","  Successfully uninstalled opencv-python-headless-4.5.5.64\n","Collecting opencv-python-headless==4.1.2.30\n","  Downloading opencv_python_headless-4.1.2.30-cp37-cp37m-manylinux1_x86_64.whl (21.8 MB)\n","\u001b[K     |████████████████████████████████| 21.8 MB 5.1 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python-headless==4.1.2.30) (1.21.6)\n","Installing collected packages: opencv-python-headless\n","Successfully installed opencv-python-headless-4.1.2.30\n","Collecting torchinfo\n","  Downloading torchinfo-1.6.5-py3-none-any.whl (21 kB)\n","Installing collected packages: torchinfo\n","Successfully installed torchinfo-1.6.5\n","Collecting timm\n","  Downloading timm-0.5.4-py3-none-any.whl (431 kB)\n","\u001b[K     |████████████████████████████████| 431 kB 5.0 MB/s \n","\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.12.0+cu113)\n","Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.11.0+cu113)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (4.2.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (2.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.21.6)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (2021.10.8)\n","Installing collected packages: timm\n","Successfully installed timm-0.5.4\n","Collecting einops==0.3.0\n","  Downloading einops-0.3.0-py2.py3-none-any.whl (25 kB)\n","Installing collected packages: einops\n","Successfully installed einops-0.3.0\n"]}],"source":["!pip install haven-ai\n","!pip install kornia\n","!pip uninstall opencv-python-headless==4.5.5.62 -y\n","!pip install opencv-python-headless==4.1.2.30\n","!pip install torchinfo\n","!pip install timm\n","!pip install einops==0.3.0"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t8EncnDsD11D","outputId":"3c3ce74b-1fe2-46d3-c5ca-bf46b5fac4a4","executionInfo":{"status":"ok","timestamp":1652343604974,"user_tz":-480,"elapsed":10364,"user":{"displayName":"IPSAL Lab-HUST","userId":"14403031417329069761"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install --upgrade --quiet git+https://github.com/haven-ai/haven-ai"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"YbJkcvCyD7bx","executionInfo":{"status":"ok","timestamp":1652343609306,"user_tz":-480,"elapsed":4341,"user":{"displayName":"IPSAL Lab-HUST","userId":"14403031417329069761"}}},"outputs":[],"source":["import tqdm.notebook as tqdm\n","import os\n","import torch\n","import numpy as np\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","from torch.utils.data import DataLoader\n","\n","from haven import haven_examples as he\n","from haven import haven_wizard as hw\n","from haven import haven_jupyter as hj\n","from haven import haven_results as hr\n","from haven import haven_utils as hu\n","\n","# from src import models\n","# from src import datasets\n","# from src import utils as ut"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"jtFCb1R_tlBI","executionInfo":{"status":"ok","timestamp":1652343609306,"user_tz":-480,"elapsed":3,"user":{"displayName":"IPSAL Lab-HUST","userId":"14403031417329069761"}}},"outputs":[],"source":["from haven import haven_chk as hc\n","# from haven import haven_results as hr\n","# from haven import haven_utils as hu\n","import torch\n","import torchvision\n","import tqdm\n","import pandas as pd\n","import pprint\n","import itertools\n","import os\n","import pylab as plt\n","import time\n","import numpy as np\n","\n","from src import *\n","import argparse\n","\n","from torch.utils.data import sampler\n","from torch.utils.data.sampler import RandomSampler\n","from torch.backends import cudnn\n","from torch.nn import functional as F\n","from torch.utils.data import DataLoader\n","# from torchvision import models\n","from torchinfo import summary\n","\n","cudnn.benchmark = True"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"ipnI6S1p5TRl","executionInfo":{"status":"ok","timestamp":1652343635997,"user_tz":-480,"elapsed":26694,"user":{"displayName":"IPSAL Lab-HUST","userId":"14403031417329069761"}}},"outputs":[],"source":["from src import models\n","from src import datasets\n","from src import utils as ut"]},{"cell_type":"markdown","metadata":{"id":"NuXo68p9Y8wL"},"source":["### Step 3: Define List of Experiments"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"N8OPEkm_fn-R","executionInfo":{"status":"ok","timestamp":1652343635998,"user_tz":-480,"elapsed":22,"user":{"displayName":"IPSAL Lab-HUST","userId":"14403031417329069761"}}},"outputs":[],"source":["# # Define a list of experiments\n","# exp_list = [{'batch_size': 8,\n","#           'dataset': {'n_classes': 2, 'name': 'c'},\n","#           'dataset_size': {'train': 'all', 'val': 'all'},\n","#           'lr': 0.00001,\n","#           'max_epoch': 101,\n","#           'model': {'base': 'fcn8_vgg16',\n","#                     'loss': 'cons_point_loss',\n","#                     'n_channels': 3,\n","#                     'n_classes': 1,\n","#                     'name': 'semseg'},\n","#           'num_channels': 1,\n","#           'optimizer': 'adam'}]"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"J4sWbKoquD_9","executionInfo":{"status":"ok","timestamp":1652343635998,"user_tz":-480,"elapsed":21,"user":{"displayName":"IPSAL Lab-HUST","userId":"14403031417329069761"}}},"outputs":[],"source":["# !python3 '/content/drive/MyDrive/deepfish/affinity_lcfcn/trainval.py' -e weakly_JCUfish_aff -sb '/content/seg_checkpoint/' -d '/content/drive/MyDrive/deepfish/Dataset/DeepFish/' -r 1 -nw 2"]},{"cell_type":"markdown","metadata":{"id":"Cnxea1-y299p"},"source":["For Results 1:"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"ZT9V5jn1vKMc","executionInfo":{"status":"ok","timestamp":1652343635999,"user_tz":-480,"elapsed":22,"user":{"displayName":"IPSAL Lab-HUST","userId":"14403031417329069761"}}},"outputs":[],"source":["# # # Define a list of experiments\n","# exp_list = [{'batch_size': 1,\n","#           'dataset': {'n_classes': 2, 'name': 'JcuFish'},\n","#           'dataset_size': {'train': 'all', 'val': 'all'},\n","#           'lr': 0.00001,\n","#           'max_epoch': 101,\n","#           'model': {'base': 'fcn8_wide_resnet50',\n","#                     'loss': 'cross_entropy',\n","#                     'n_channels': 3,\n","#                     'n_classes': 2,\n","#                     'name': 'semseg',\n","#                     'with_PASPP': False},\n","#           'num_channels': 1,\n","#           'optimizer': 'adam'}]"]},{"cell_type":"markdown","source":["Results2"],"metadata":{"id":"DhGxEn1jonR-"}},{"cell_type":"code","source":["# # # Define a list of experiments\n","# exp_list = [{'batch_size': 1,\n","#           'dataset': {'n_classes': 2, 'name': 'JcuFish'},\n","#           'dataset_size': {'train': 'all', 'val': 'all'},\n","#           'lr': 0.00001,\n","#           'max_epoch': 31,\n","#           'model': {'base': 'fcn8_vgg16',\n","#                     'loss': 'cross_entropy',\n","#                     'n_channels': 3,\n","#                     'n_classes': 2,\n","#                     'name': 'semseg',\n","#                     'with_PASPP': True},\n","#           'num_channels': 1,\n","#           'optimizer': 'adam'}]"],"metadata":{"id":"yPqqdILroo-h","executionInfo":{"status":"ok","timestamp":1652344190758,"user_tz":-480,"elapsed":281,"user":{"displayName":"IPSAL Lab-HUST","userId":"14403031417329069761"}}},"execution_count":28,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qj140Av728-x"},"source":["Atempt"]},{"cell_type":"code","source":["# worse\n","# Define a list of experiments\n","exp_list = [{'batch_size': 1,\n","          'dataset': {'n_classes': 2, 'name': 'SumFish'},\n","          'dataset_size': {'train': 'all', 'val': 'all'},\n","          'lr': 1e-5,\n","          'max_epoch': 101,\n","          'model': {'base': 'fcn8_vgg16',\n","                    'loss': 'cross_entropy',\n","                    'n_channels': 3,\n","                    'n_classes': 2,\n","                    'name': 'semseg',\n","                    'with_PASPP': False},\n","          'num_channels': 1,\n","          'optimizer': 'adam'}]\n","\n"],"metadata":{"id":"KBT7TPnMDkSA","executionInfo":{"status":"ok","timestamp":1652344259973,"user_tz":-480,"elapsed":290,"user":{"displayName":"IPSAL Lab-HUST","userId":"14403031417329069761"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["# # # Define a list of experiments\n","# exp_list = [{'batch_size': 1,\n","#           'dataset': {'n_classes': 2, 'name': 'SumFish'},\n","#           'dataset_size': {'train': 'all', 'val': 'all'},\n","#           'lr': 1e-4,\n","#           'max_epoch': 31,\n","#           'model': {'base': 'fcn8_vgg16',\n","#                     'loss': 'cons_point_loss',\n","#                     'n_channels': 3,\n","#                     'n_classes': 2,\n","#                     'name': 'semseg'},\n","#           'num_channels': 1,\n","#           'optimizer': 'adam'}]"],"metadata":{"id":"bqIlq1WDt9l4","executionInfo":{"status":"ok","timestamp":1652343635999,"user_tz":-480,"elapsed":21,"user":{"displayName":"IPSAL Lab-HUST","userId":"14403031417329069761"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["### Prepare data"],"metadata":{"id":"SbRePBdyhDuB"}},{"cell_type":"code","source":["'''\n","from skimage.io import imread,imsave\n","import matplotlib.pyplot as plt\n","image_path='/content/drive/MyDrive/deepfish/Dataset/DeepFish/train_val/images/d_r_31_.jpg'\n","image_raw = imread(image_path)\n","plt.imshow(image_raw)\n","# image_path='/content/drive/MyDrive/deepfish/Dataset/DeepFish/train_val/masks/d_r_31_.bmp'\n","# mask = imread(image_path)\n","# plt.imshow(mask)\n","'''"],"metadata":{"id":"tt8Oagxpomsg","colab":{"base_uri":"https://localhost:8080/","height":72},"executionInfo":{"status":"ok","timestamp":1652343636000,"user_tz":-480,"elapsed":22,"user":{"displayName":"IPSAL Lab-HUST","userId":"14403031417329069761"}},"outputId":"a172f37c-19a9-451d-9716-9d334b16d55f"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\nfrom skimage.io import imread,imsave\\nimport matplotlib.pyplot as plt\\nimage_path='/content/drive/MyDrive/deepfish/Dataset/DeepFish/train_val/images/d_r_31_.jpg'\\nimage_raw = imread(image_path)\\nplt.imshow(image_raw)\\n# image_path='/content/drive/MyDrive/deepfish/Dataset/DeepFish/train_val/masks/d_r_31_.bmp'\\n# mask = imread(image_path)\\n# plt.imshow(mask)\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["   '''\n","   \n","    imw, imh = mask.shape[0], mask.shape[1]\n","    # Human = np.zeros((imw, imh))\n","    # Robot = np.zeros((imw, imh))\n","    Fish = np.zeros((imw, imh))\n","    # Reef = np.zeros((imw, imh))\n","    # Wreck = np.zeros((imw, imh))\n","    for i in range(imw):\n","        for j in range(imh):\n","          if (mask[i,j,0]==255 and mask[i,j,1]==255 and mask[i,j,2]==0):\n","            Fish[i, j] = 1\n","          else: pass  \n","\n","                   # if (mask[i,j,0]==0 and mask[i,j,1]==0 and mask[i,j,2]==255):\n","            #     Human[i, j] = 1 \n","            # elif (mask[i,j,0]==255 and mask[i,j,1]==0 and mask[i,j,2]==0):\n","            #     Robot[i, j] = 1  \n","          # if (mask[i,j,0]==255 and mask[i,j,1]==255 and mask[i,j,2]==0):\n","          #       Fish[i, j] = 1  \n","            # elif (mask[i,j,0]==255 and mask[i,j,1]==0 and mask[i,j,2]==1):\n","            #     Reef[i, j] = 1  \n","            # elif (mask[i,j,0]==0 and mask[i,j,1]==255 and mask[i,j,2]==255):\n","            #     Wreck[i, j] = 1  \n","          # else: pass\n","          # plt.imshow(Fish,cmap='gray')\n","\n","          '''"],"metadata":{"id":"kPZw8QIxSFZv","colab":{"base_uri":"https://localhost:8080/","height":127},"executionInfo":{"status":"ok","timestamp":1652343636000,"user_tz":-480,"elapsed":21,"user":{"displayName":"IPSAL Lab-HUST","userId":"14403031417329069761"}},"outputId":"aad28d93-dfed-459c-e209-0389fa3439e6"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\n\\n imw, imh = mask.shape[0], mask.shape[1]\\n # Human = np.zeros((imw, imh))\\n # Robot = np.zeros((imw, imh))\\n Fish = np.zeros((imw, imh))\\n # Reef = np.zeros((imw, imh))\\n # Wreck = np.zeros((imw, imh))\\n for i in range(imw):\\n     for j in range(imh):\\n       if (mask[i,j,0]==255 and mask[i,j,1]==255 and mask[i,j,2]==0):\\n         Fish[i, j] = 1\\n       else: pass  \\n\\n                # if (mask[i,j,0]==0 and mask[i,j,1]==0 and mask[i,j,2]==255):\\n         #     Human[i, j] = 1 \\n         # elif (mask[i,j,0]==255 and mask[i,j,1]==0 and mask[i,j,2]==0):\\n         #     Robot[i, j] = 1  \\n       # if (mask[i,j,0]==255 and mask[i,j,1]==255 and mask[i,j,2]==0):\\n       #       Fish[i, j] = 1  \\n         # elif (mask[i,j,0]==255 and mask[i,j,1]==0 and mask[i,j,2]==1):\\n         #     Reef[i, j] = 1  \\n         # elif (mask[i,j,0]==0 and mask[i,j,1]==255 and mask[i,j,2]==255):\\n         #     Wreck[i, j] = 1  \\n       # else: pass\\n       # plt.imshow(Fish,cmap='gray')\\n\\n       \""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":[""],"metadata":{"id":"IQnJXc20SNz4","executionInfo":{"status":"ok","timestamp":1652343636000,"user_tz":-480,"elapsed":21,"user":{"displayName":"IPSAL Lab-HUST","userId":"14403031417329069761"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["'''\n","\n","from PIL import Image\n","from skimage.io import imread,imsave\n","import cv2\n","def get_data(train_path, mask_path,save_img_path,save_mask_path, size=(256, 256)):\n","    from tqdm import tqdm\n","    images, masks = [], []\n","    for idx in tqdm(range(len(train_path))):\n","        # image1 = Image.open(train_path[idx])\n","        # image = Image.open(train_path[idx]).resize((size[1], size[0]))\n","        # mask = Image.open(mask_path[idx]).resize((size[1], size[0]), Image.NEAREST)\n","        mask1 = Image.open(mask_path[idx])\n","        mask1.save('/content/temp.bmp')\n","        mask1=imread('/content/temp.bmp')\n","        # mask1 = np.asarray(mask, dtype=np.uint8)\n","        imw, imh = mask1.shape[0], mask1.shape[1]\n","        # imw, imh = mask.size[0], mask.size[1]\n","        Fish = np.zeros((imw, imh))\n","        for i in range(imw):\n","          for j in range(imh):\n","            if (mask1[i,j,0]==255 and mask1[i,j,1]==255 and mask1[i,j,2]==0):\n","              # Fish[i,j] = 1\n","              Fish[i,j] = 255\n","            else: pass \n","        # cv2.imwrite(save_mask_path+'temp.png',Fish)\n","        # cv2.imwrite('/content/temp.bmp',Fish)\n","        # temp_img = Image.open('/content/temp.bmp')\n","        # temp_img.save(save_mask_path+str(idx)+'.bmp')\n","        imsave(save_mask_path+str(idx)+'.bmp',Fish)\n","        # mask = np.asarray(mask, dtype=np.uint8)\n","        # mask.save(save_mask_path[idx])\n","        # images.append(image)\n","        # masks.append(mask)\n","    # return np.stack(images), np.stack(masks)\n","\n"," '''   "],"metadata":{"id":"zN0nWL38Xclp","colab":{"base_uri":"https://localhost:8080/","height":145},"executionInfo":{"status":"ok","timestamp":1652343636001,"user_tz":-480,"elapsed":21,"user":{"displayName":"IPSAL Lab-HUST","userId":"14403031417329069761"}},"outputId":"b87dc049-c2f7-41cf-96e9-44e9be2a6f8c"},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\n\\nfrom PIL import Image\\nfrom skimage.io import imread,imsave\\nimport cv2\\ndef get_data(train_path, mask_path,save_img_path,save_mask_path, size=(256, 256)):\\n    from tqdm import tqdm\\n    images, masks = [], []\\n    for idx in tqdm(range(len(train_path))):\\n        # image1 = Image.open(train_path[idx])\\n        # image = Image.open(train_path[idx]).resize((size[1], size[0]))\\n        # mask = Image.open(mask_path[idx]).resize((size[1], size[0]), Image.NEAREST)\\n        mask1 = Image.open(mask_path[idx])\\n        mask1.save('/content/temp.bmp')\\n        mask1=imread('/content/temp.bmp')\\n        # mask1 = np.asarray(mask, dtype=np.uint8)\\n        imw, imh = mask1.shape[0], mask1.shape[1]\\n        # imw, imh = mask.size[0], mask.size[1]\\n        Fish = np.zeros((imw, imh))\\n        for i in range(imw):\\n          for j in range(imh):\\n            if (mask1[i,j,0]==255 and mask1[i,j,1]==255 and mask1[i,j,2]==0):\\n              # Fish[i,j] = 1\\n              Fish[i,j] = 255\\n            else: pass \\n        # cv2.imwrite(save_mask_path+'temp.png',Fish)\\n        # cv2.imwrite('/content/temp.bmp',Fish)\\n        # temp_img = Image.open('/content/temp.bmp')\\n        # temp_img.save(save_mask_path+str(idx)+'.bmp')\\n        imsave(save_mask_path+str(idx)+'.bmp',Fish)\\n        # mask = np.asarray(mask, dtype=np.uint8)\\n        # mask.save(save_mask_path[idx])\\n        # images.append(image)\\n        # masks.append(mask)\\n    # return np.stack(images), np.stack(masks)\\n\\n \""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["'''\n","i mport glob \n","train_path = glob.glob(\"/content/drive/MyDrive/deepfish/Dataset/DeepFish/train_val_org/images/*.jpg\")\n","save_img_path=\"/content/drive/MyDrive/deepfish/Dataset/DeepFish/train_val/images/\"\n","\n","mask_path= glob.glob(\"/content/drive/MyDrive/deepfish/Dataset/DeepFish/train_val_org/masks/*.bmp\")\n","save_mask_path=\"/content/drive/MyDrive/deepfish/Dataset/DeepFish/train_val/masks/\"\n","\n","get_data(train_path, mask_path,save_img_path,save_mask_path, size=(256, 256))\n","\n","train_path = glob.glob(\"/content/drive/MyDrive/deepfish/Dataset/DeepFish/TEST_org/images/*.jpg\")\n","save_img_path=\"/content/drive/MyDrive/deepfish/Dataset/DeepFish/TEST/images/\"\n","\n","mask_path= glob.glob(\"/content/drive/MyDrive/deepfish/Dataset/DeepFish/TEST_org/masks/*.bmp\")\n","save_mask_path=\"/content/drive/MyDrive/deepfish/Dataset/DeepFish/TEST/masks/\"\n","\n","get_data(train_path, mask_path,save_img_path,save_mask_path, size=(256, 256))\n","'''"],"metadata":{"id":"TipEgYz5YJbs","colab":{"base_uri":"https://localhost:8080/","height":127},"executionInfo":{"status":"ok","timestamp":1652343636001,"user_tz":-480,"elapsed":21,"user":{"displayName":"IPSAL Lab-HUST","userId":"14403031417329069761"}},"outputId":"bcb4a3a5-4b67-4269-e761-d3c5c80ea70c"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\ni mport glob \\ntrain_path = glob.glob(\"/content/drive/MyDrive/deepfish/Dataset/DeepFish/train_val_org/images/*.jpg\")\\nsave_img_path=\"/content/drive/MyDrive/deepfish/Dataset/DeepFish/train_val/images/\"\\n\\nmask_path= glob.glob(\"/content/drive/MyDrive/deepfish/Dataset/DeepFish/train_val_org/masks/*.bmp\")\\nsave_mask_path=\"/content/drive/MyDrive/deepfish/Dataset/DeepFish/train_val/masks/\"\\n\\nget_data(train_path, mask_path,save_img_path,save_mask_path, size=(256, 256))\\n\\ntrain_path = glob.glob(\"/content/drive/MyDrive/deepfish/Dataset/DeepFish/TEST_org/images/*.jpg\")\\nsave_img_path=\"/content/drive/MyDrive/deepfish/Dataset/DeepFish/TEST/images/\"\\n\\nmask_path= glob.glob(\"/content/drive/MyDrive/deepfish/Dataset/DeepFish/TEST_org/masks/*.bmp\")\\nsave_mask_path=\"/content/drive/MyDrive/deepfish/Dataset/DeepFish/TEST/masks/\"\\n\\nget_data(train_path, mask_path,save_img_path,save_mask_path, size=(256, 256))\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":[""],"metadata":{"id":"JEriNg5zlEOv","executionInfo":{"status":"ok","timestamp":1652343636001,"user_tz":-480,"elapsed":20,"user":{"displayName":"IPSAL Lab-HUST","userId":"14403031417329069761"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["# idx=31\n","# ask_path= glob.glob(\"/content/drive/MyDrive/deepfish/Dataset/DeepFish/train_val/masks/*.bmp\")\n","# mask1 = Image.open(mask_path[idx])\n","# plt.imshow(mask1)\n","# mask1.save('/content/temp.bmp')"],"metadata":{"id":"ZYUwYcTalS33","executionInfo":{"status":"ok","timestamp":1652343636001,"user_tz":-480,"elapsed":20,"user":{"displayName":"IPSAL Lab-HUST","userId":"14403031417329069761"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["# idx=31\n","# ask_path= glob.glob(\"/content/drive/MyDrive/deepfish/Dataset/DeepFish/Segmentation/masks/valid/*.png\")\n","# mask1 = Image.open(mask_path[idx])\n","# plt.imshow(mask1)\n","# mask1.save('/content/temp.bmp')\n","# # mask1.max()\n","# mask1.sum()"],"metadata":{"id":"NNs8h3Z1IoI3","executionInfo":{"status":"ok","timestamp":1652343636002,"user_tz":-480,"elapsed":21,"user":{"displayName":"IPSAL Lab-HUST","userId":"14403031417329069761"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["# idx=31\n","# ask_path= glob.glob(\"/content/drive/MyDrive/deepfish/Dataset/DeepFish/Segmentation/masks/empty/*.png\")\n","# mask1 = Image.open(mask_path[idx])\n","# plt.imshow(mask1)\n","# mask1.save('/content/temp.bmp')\n","# mask1.max()\n","# mask1.sum()"],"metadata":{"id":"zDS5sKjkO2Pn","executionInfo":{"status":"ok","timestamp":1652343636002,"user_tz":-480,"elapsed":20,"user":{"displayName":"IPSAL Lab-HUST","userId":"14403031417329069761"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["# imw, imh = mask1.shape[0], mask1.shape[1]\n","# # imw, imh = mask.size[0], mask.size[1]\n","# Fish = np.zeros((imw, imh))\n","# for i in range(imw):\n","#   for j in range(imh):\n","#     if (mask1[i,j,0]==255 and mask1[i,j,1]==255 and mask1[i,j,2]==0):\n","#       Fish[i,j] = 255\n","#     else: pass \n","\n","# plt.imshow(Fish,cmap='gray')    \n","# mask3=imread('/content/temp.bmp')\n","# plt.imshow(mask3) \n","# imsave(save_mask_path+str(idx)+'.bmp',mask3)"],"metadata":{"id":"ONFwXDpClxUW","executionInfo":{"status":"ok","timestamp":1652343636002,"user_tz":-480,"elapsed":20,"user":{"displayName":"IPSAL Lab-HUST","userId":"14403031417329069761"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"ty3sRa5ck9fz","executionInfo":{"status":"ok","timestamp":1652343636002,"user_tz":-480,"elapsed":20,"user":{"displayName":"IPSAL Lab-HUST","userId":"14403031417329069761"}}},"execution_count":24,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7FxWbMK0ZEps"},"source":["### Step 4: Train and Validate"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"q5Xe_rH27p16","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"error","timestamp":1652344334231,"user_tz":-480,"elapsed":28507,"user":{"displayName":"IPSAL Lab-HUST","userId":"14403031417329069761"}},"outputId":"6ee5ea7a-dbf5-4383-a237-86d8f2eea33e"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Running 1 experiments\n","\n","******\n","Haven: 0.6.7\n","Exp id: c51c0535acae0b5dc28f58ce8f11d75f\n","\n","Hyperparameters:\n","----------------\n","{'batch_size': 1,\n"," 'dataset': {'n_classes': 2, 'name': 'SumFish'},\n"," 'dataset_size': {'train': 'all', 'val': 'all'},\n"," 'lr': 1e-05,\n"," 'max_epoch': 101,\n"," 'model': {'base': 'fcn8_vgg16',\n","           'loss': 'cross_entropy',\n","           'n_channels': 3,\n","           'n_classes': 2,\n","           'name': 'semseg',\n","           'with_PASPP': False},\n"," 'num_channels': 1,\n"," 'optimizer': 'adam'}\n","\n","Save directory: /content/drive/.shortcut-targets-by-id/1-lqIvGv8GnqHULF2dwocEGdE1OZTOoFv/deepfish/affinity_lcfcn/results/tmp/c51c0535acae0b5dc28f58ce8f11d75f\n","====================================================================================================\n","{'batch_size': 1,\n"," 'dataset': {'n_classes': 2, 'name': 'SumFish'},\n"," 'dataset_size': {'train': 'all', 'val': 'all'},\n"," 'lr': 1e-05,\n"," 'max_epoch': 101,\n"," 'model': {'base': 'fcn8_vgg16',\n","           'loss': 'cross_entropy',\n","           'n_channels': 3,\n","           'n_classes': 2,\n","           'name': 'semseg',\n","           'with_PASPP': False},\n"," 'num_channels': 1,\n"," 'optimizer': 'adam'}\n","Experiment saved in /content/drive/.shortcut-targets-by-id/1-lqIvGv8GnqHULF2dwocEGdE1OZTOoFv/deepfish/affinity_lcfcn/results/tmp/c51c0535acae0b5dc28f58ce8f11d75f/c51c0535acae0b5dc28f58ce8f11d75f\n","-----done-torch-cuda-seed-----\n","train : 1535 , with_fish: 1535 , without_fish: 0\n","train : 1535\n","val : 110 , with_fish: 110 , without_fish: 0\n","val : 110\n","test : 110 , with_fish: 110 , without_fish: 0\n","test : 110\n","-----done-loader-dts-----\n"]},{"output_type":"stream","name":"stderr","text":["\n","\n","  0%|          | 0/110 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n","  1%|          | 1/110 [00:00<00:13,  8.26it/s]\u001b[A\u001b[A\n","\n","  2%|▏         | 2/110 [00:00<00:15,  7.01it/s]\u001b[A\u001b[A\n","\n","  3%|▎         | 3/110 [00:00<00:14,  7.57it/s]\u001b[A\u001b[A\n","\n","  4%|▎         | 4/110 [00:00<00:13,  7.58it/s]\u001b[A\u001b[A\n","\n","  5%|▍         | 5/110 [00:00<00:12,  8.21it/s]\u001b[A\u001b[A\n","\n","  5%|▌         | 6/110 [00:00<00:12,  8.59it/s]\u001b[A\u001b[A\n","\n","  6%|▋         | 7/110 [00:00<00:12,  8.23it/s]\u001b[A\u001b[A\n","\n","  8%|▊         | 9/110 [00:01<00:11,  9.13it/s]\u001b[A\u001b[A\n","\n"," 10%|█         | 11/110 [00:01<00:10,  9.89it/s]\u001b[A\u001b[A\n","\n"," 12%|█▏        | 13/110 [00:01<00:09, 10.23it/s]\u001b[A\u001b[A\n","\n"," 14%|█▎        | 15/110 [00:01<00:08, 10.88it/s]\u001b[A\u001b[A\n","\n"," 15%|█▌        | 17/110 [00:01<00:08, 10.81it/s]\u001b[A\u001b[A\n","\n"," 17%|█▋        | 19/110 [00:01<00:08, 11.00it/s]\u001b[A\u001b[A\n","\n"," 19%|█▉        | 21/110 [00:02<00:07, 11.29it/s]\u001b[A\u001b[A\n","\n"," 21%|██        | 23/110 [00:02<00:07, 11.58it/s]\u001b[A\u001b[A\n","\n"," 23%|██▎       | 25/110 [00:02<00:07, 11.35it/s]\u001b[A\u001b[A\n","\n"," 25%|██▍       | 27/110 [00:02<00:07, 11.13it/s]\u001b[A\u001b[A\n","\n"," 26%|██▋       | 29/110 [00:02<00:07, 11.24it/s]\u001b[A\u001b[A\n","\n"," 28%|██▊       | 31/110 [00:03<00:06, 11.33it/s]\u001b[A\u001b[A\n","\n"," 30%|███       | 33/110 [00:03<00:06, 11.34it/s]\u001b[A\u001b[A\n","\n"," 32%|███▏      | 35/110 [00:03<00:06, 11.51it/s]\u001b[A\u001b[A\n","\n"," 34%|███▎      | 37/110 [00:03<00:06, 11.34it/s]\u001b[A\u001b[A\n","\n"," 35%|███▌      | 39/110 [00:03<00:06, 11.40it/s]\u001b[A\u001b[A\n","\n"," 37%|███▋      | 41/110 [00:03<00:06, 11.33it/s]\u001b[A\u001b[A\n","\n"," 39%|███▉      | 43/110 [00:04<00:05, 11.28it/s]\u001b[A\u001b[A\n","\n"," 41%|████      | 45/110 [00:04<00:05, 10.98it/s]\u001b[A\u001b[A\n","\n"," 43%|████▎     | 47/110 [00:04<00:05, 10.72it/s]\u001b[A\u001b[A\n","\n"," 45%|████▍     | 49/110 [00:04<00:05, 10.88it/s]\u001b[A\u001b[A\n","\n"," 46%|████▋     | 51/110 [00:04<00:05, 10.63it/s]\u001b[A\u001b[A\n","\n"," 48%|████▊     | 53/110 [00:04<00:05, 10.87it/s]\u001b[A\u001b[A\n","\n"," 50%|█████     | 55/110 [00:05<00:04, 11.08it/s]\u001b[A\u001b[A\n","\n"," 52%|█████▏    | 57/110 [00:05<00:04, 11.15it/s]\u001b[A\u001b[A\n","\n"," 54%|█████▎    | 59/110 [00:05<00:04, 11.39it/s]\u001b[A\u001b[A\n","\n"," 55%|█████▌    | 61/110 [00:05<00:04, 11.46it/s]\u001b[A\u001b[A\n","\n"," 57%|█████▋    | 63/110 [00:05<00:04, 11.55it/s]\u001b[A\u001b[A\n","\n"," 59%|█████▉    | 65/110 [00:06<00:03, 11.67it/s]\u001b[A\u001b[A\n","\n"," 61%|██████    | 67/110 [00:06<00:03, 11.74it/s]\u001b[A\u001b[A\n","\n"," 63%|██████▎   | 69/110 [00:06<00:03, 11.85it/s]\u001b[A\u001b[A\n","\n"," 65%|██████▍   | 71/110 [00:06<00:03, 11.84it/s]\u001b[A\u001b[A\n","\n"," 66%|██████▋   | 73/110 [00:06<00:03, 11.45it/s]\u001b[A\u001b[A\n","\n"," 68%|██████▊   | 75/110 [00:06<00:03, 11.33it/s]\u001b[A\u001b[A\n","\n"," 70%|███████   | 77/110 [00:07<00:02, 11.40it/s]\u001b[A\u001b[A\n","\n"," 72%|███████▏  | 79/110 [00:07<00:02, 11.26it/s]\u001b[A\u001b[A\n","\n"," 74%|███████▎  | 81/110 [00:07<00:02, 11.50it/s]\u001b[A\u001b[A\n","\n"," 75%|███████▌  | 83/110 [00:07<00:02, 11.52it/s]\u001b[A\u001b[A\n","\n"," 77%|███████▋  | 85/110 [00:07<00:02, 11.74it/s]\u001b[A\u001b[A\n","\n"," 79%|███████▉  | 87/110 [00:07<00:01, 11.53it/s]\u001b[A\u001b[A\n","\n"," 81%|████████  | 89/110 [00:08<00:01, 11.71it/s]\u001b[A\u001b[A\n","\n"," 83%|████████▎ | 91/110 [00:08<00:01, 11.83it/s]\u001b[A\u001b[A\n","\n"," 85%|████████▍ | 93/110 [00:08<00:01, 11.99it/s]\u001b[A\u001b[A\n","\n"," 86%|████████▋ | 95/110 [00:08<00:01, 11.96it/s]\u001b[A\u001b[A\n","\n"," 88%|████████▊ | 97/110 [00:08<00:01, 12.07it/s]\u001b[A\u001b[A\n","\n"," 90%|█████████ | 99/110 [00:08<00:00, 12.16it/s]\u001b[A\u001b[A\n","\n"," 92%|█████████▏| 101/110 [00:09<00:00, 12.20it/s]\u001b[A\u001b[A\n","\n"," 94%|█████████▎| 103/110 [00:09<00:00, 12.20it/s]\u001b[A\u001b[A\n","\n"," 95%|█████████▌| 105/110 [00:09<00:00, 12.07it/s]\u001b[A\u001b[A\n","\n"," 97%|█████████▋| 107/110 [00:09<00:00, 12.11it/s]\u001b[A\u001b[A\n","\n","100%|██████████| 110/110 [00:09<00:00, 11.19it/s]\n","\n","\n","Training:   0%|          | 0/220 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.693:   0%|          | 0/220 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.693:   0%|          | 1/220 [00:00<00:42,  5.12it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.692:   0%|          | 1/220 [00:00<00:42,  5.12it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.692:   1%|          | 2/220 [00:00<00:42,  5.08it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.691:   1%|          | 2/220 [00:00<00:42,  5.08it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.691:   1%|▏         | 3/220 [00:00<00:43,  5.03it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.689:   1%|▏         | 3/220 [00:00<00:43,  5.03it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.689:   2%|▏         | 4/220 [00:00<00:42,  5.09it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.688:   2%|▏         | 4/220 [00:00<00:42,  5.09it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.688:   2%|▏         | 5/220 [00:00<00:42,  5.08it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.686:   2%|▏         | 5/220 [00:01<00:42,  5.08it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.686:   3%|▎         | 6/220 [00:01<00:42,  5.07it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.685:   3%|▎         | 6/220 [00:01<00:42,  5.07it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.685:   3%|▎         | 7/220 [00:01<00:41,  5.08it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.683:   3%|▎         | 7/220 [00:01<00:41,  5.08it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.683:   4%|▎         | 8/220 [00:01<00:41,  5.11it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.681:   4%|▎         | 8/220 [00:01<00:41,  5.11it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.681:   4%|▍         | 9/220 [00:01<00:40,  5.20it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.679:   4%|▍         | 9/220 [00:01<00:40,  5.20it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.679:   5%|▍         | 10/220 [00:01<00:40,  5.14it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.677:   5%|▍         | 10/220 [00:02<00:40,  5.14it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.677:   5%|▌         | 11/220 [00:02<00:40,  5.13it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.674:   5%|▌         | 11/220 [00:02<00:40,  5.13it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.674:   5%|▌         | 12/220 [00:02<00:41,  5.00it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.672:   5%|▌         | 12/220 [00:02<00:41,  5.00it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.672:   6%|▌         | 13/220 [00:02<00:41,  4.93it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.669:   6%|▌         | 13/220 [00:02<00:41,  4.93it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.669:   6%|▋         | 14/220 [00:02<00:41,  5.00it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.666:   6%|▋         | 14/220 [00:02<00:41,  5.00it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.666:   7%|▋         | 15/220 [00:02<00:40,  5.01it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.663:   7%|▋         | 15/220 [00:03<00:40,  5.01it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.663:   7%|▋         | 16/220 [00:03<00:41,  4.97it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.660:   7%|▋         | 16/220 [00:03<00:41,  4.97it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.660:   8%|▊         | 17/220 [00:03<00:40,  5.03it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.657:   8%|▊         | 17/220 [00:03<00:40,  5.03it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.657:   8%|▊         | 18/220 [00:03<00:39,  5.08it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.653:   8%|▊         | 18/220 [00:03<00:39,  5.08it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.653:   9%|▊         | 19/220 [00:03<00:39,  5.13it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.649:   9%|▊         | 19/220 [00:03<00:39,  5.13it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.649:   9%|▉         | 20/220 [00:03<00:39,  5.06it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.645:   9%|▉         | 20/220 [00:04<00:39,  5.06it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.645:  10%|▉         | 21/220 [00:04<00:39,  5.04it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.641:  10%|▉         | 21/220 [00:04<00:39,  5.04it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.641:  10%|█         | 22/220 [00:04<00:39,  5.05it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.637:  10%|█         | 22/220 [00:04<00:39,  5.05it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.637:  10%|█         | 23/220 [00:04<00:39,  4.95it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.632:  10%|█         | 23/220 [00:04<00:39,  4.95it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.632:  11%|█         | 24/220 [00:04<00:39,  5.00it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.628:  11%|█         | 24/220 [00:04<00:39,  5.00it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.628:  11%|█▏        | 25/220 [00:04<00:39,  4.98it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.623:  11%|█▏        | 25/220 [00:05<00:39,  4.98it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.623:  12%|█▏        | 26/220 [00:05<00:39,  4.96it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.617:  12%|█▏        | 26/220 [00:05<00:39,  4.96it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.617:  12%|█▏        | 27/220 [00:05<00:39,  4.93it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.612:  12%|█▏        | 27/220 [00:05<00:39,  4.93it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.612:  13%|█▎        | 28/220 [00:05<00:39,  4.87it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.606:  13%|█▎        | 28/220 [00:05<00:39,  4.87it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.606:  13%|█▎        | 29/220 [00:05<00:38,  4.92it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.600:  13%|█▎        | 29/220 [00:05<00:38,  4.92it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.600:  14%|█▎        | 30/220 [00:05<00:38,  4.98it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.594:  14%|█▎        | 30/220 [00:06<00:38,  4.98it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.594:  14%|█▍        | 31/220 [00:06<00:37,  5.02it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.588:  14%|█▍        | 31/220 [00:06<00:37,  5.02it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.588:  15%|█▍        | 32/220 [00:06<00:37,  5.00it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.581:  15%|█▍        | 32/220 [00:06<00:37,  5.00it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.581:  15%|█▌        | 33/220 [00:06<00:37,  4.99it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.574:  15%|█▌        | 33/220 [00:06<00:37,  4.99it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.574:  15%|█▌        | 34/220 [00:06<00:38,  4.85it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.567:  15%|█▌        | 34/220 [00:06<00:38,  4.85it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.567:  16%|█▌        | 35/220 [00:06<00:38,  4.85it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.560:  16%|█▌        | 35/220 [00:07<00:38,  4.85it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.560:  16%|█▋        | 36/220 [00:07<00:38,  4.82it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.552:  16%|█▋        | 36/220 [00:07<00:38,  4.82it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.552:  17%|█▋        | 37/220 [00:07<00:37,  4.89it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.545:  17%|█▋        | 37/220 [00:07<00:37,  4.89it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.545:  17%|█▋        | 38/220 [00:07<00:37,  4.91it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.537:  17%|█▋        | 38/220 [00:07<00:37,  4.91it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.537:  18%|█▊        | 39/220 [00:07<00:36,  4.91it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.529:  18%|█▊        | 39/220 [00:08<00:36,  4.91it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.529:  18%|█▊        | 40/220 [00:08<00:36,  4.96it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.521:  18%|█▊        | 40/220 [00:08<00:36,  4.96it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.521:  19%|█▊        | 41/220 [00:08<00:36,  4.95it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.513:  19%|█▊        | 41/220 [00:08<00:36,  4.95it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.513:  19%|█▉        | 42/220 [00:08<00:35,  4.95it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.505:  19%|█▉        | 42/220 [00:08<00:35,  4.95it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.505:  20%|█▉        | 43/220 [00:08<00:35,  4.93it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.497:  20%|█▉        | 43/220 [00:08<00:35,  4.93it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.497:  20%|██        | 44/220 [00:08<00:35,  4.98it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.488:  20%|██        | 44/220 [00:09<00:35,  4.98it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.488:  20%|██        | 45/220 [00:09<00:35,  5.00it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.480:  20%|██        | 45/220 [00:09<00:35,  5.00it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.480:  21%|██        | 46/220 [00:09<00:34,  4.98it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.472:  21%|██        | 46/220 [00:09<00:34,  4.98it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.472:  21%|██▏       | 47/220 [00:09<00:34,  4.96it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.464:  21%|██▏       | 47/220 [00:09<00:34,  4.96it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.464:  22%|██▏       | 48/220 [00:09<00:35,  4.89it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.456:  22%|██▏       | 48/220 [00:09<00:35,  4.89it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.456:  22%|██▏       | 49/220 [00:09<00:34,  4.95it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.448:  22%|██▏       | 49/220 [00:10<00:34,  4.95it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.448:  23%|██▎       | 50/220 [00:10<00:35,  4.83it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.440:  23%|██▎       | 50/220 [00:10<00:35,  4.83it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.440:  23%|██▎       | 51/220 [00:10<00:34,  4.88it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.432:  23%|██▎       | 51/220 [00:10<00:34,  4.88it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.432:  24%|██▎       | 52/220 [00:10<00:34,  4.92it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.425:  24%|██▎       | 52/220 [00:10<00:34,  4.92it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.425:  24%|██▍       | 53/220 [00:10<00:34,  4.86it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.417:  24%|██▍       | 53/220 [00:10<00:34,  4.86it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.417:  25%|██▍       | 54/220 [00:10<00:34,  4.88it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.410:  25%|██▍       | 54/220 [00:11<00:34,  4.88it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.410:  25%|██▌       | 55/220 [00:11<00:33,  4.89it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.403:  25%|██▌       | 55/220 [00:11<00:33,  4.89it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.403:  25%|██▌       | 56/220 [00:11<00:33,  4.94it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.396:  25%|██▌       | 56/220 [00:11<00:33,  4.94it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.396:  26%|██▌       | 57/220 [00:11<00:33,  4.90it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.389:  26%|██▌       | 57/220 [00:11<00:33,  4.90it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.389:  26%|██▋       | 58/220 [00:11<00:32,  4.92it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.383:  26%|██▋       | 58/220 [00:11<00:32,  4.92it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.383:  27%|██▋       | 59/220 [00:11<00:32,  4.92it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.377:  27%|██▋       | 59/220 [00:12<00:32,  4.92it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.377:  27%|██▋       | 60/220 [00:12<00:32,  4.89it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.370:  27%|██▋       | 60/220 [00:12<00:32,  4.89it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.370:  28%|██▊       | 61/220 [00:12<00:32,  4.94it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.364:  28%|██▊       | 61/220 [00:12<00:32,  4.94it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.364:  28%|██▊       | 62/220 [00:12<00:32,  4.93it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.359:  28%|██▊       | 62/220 [00:12<00:32,  4.93it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.359:  29%|██▊       | 63/220 [00:12<00:31,  4.95it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.353:  29%|██▊       | 63/220 [00:12<00:31,  4.95it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.353:  29%|██▉       | 64/220 [00:12<00:31,  4.96it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.348:  29%|██▉       | 64/220 [00:13<00:31,  4.96it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.348:  30%|██▉       | 65/220 [00:13<00:31,  4.88it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.342:  30%|██▉       | 65/220 [00:13<00:31,  4.88it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.342:  30%|███       | 66/220 [00:13<00:31,  4.92it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.337:  30%|███       | 66/220 [00:13<00:31,  4.92it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.337:  30%|███       | 67/220 [00:13<00:30,  4.96it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.332:  30%|███       | 67/220 [00:13<00:30,  4.96it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.332:  31%|███       | 68/220 [00:13<00:30,  4.90it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.328:  31%|███       | 68/220 [00:13<00:30,  4.90it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.328:  31%|███▏      | 69/220 [00:13<00:30,  4.92it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.323:  31%|███▏      | 69/220 [00:14<00:30,  4.92it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.323:  32%|███▏      | 70/220 [00:14<00:31,  4.79it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.318:  32%|███▏      | 70/220 [00:14<00:31,  4.79it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.318:  32%|███▏      | 71/220 [00:14<00:30,  4.83it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.314:  32%|███▏      | 71/220 [00:14<00:30,  4.83it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.314:  33%|███▎      | 72/220 [00:14<00:30,  4.88it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.310:  33%|███▎      | 72/220 [00:14<00:30,  4.88it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.310:  33%|███▎      | 73/220 [00:14<00:30,  4.85it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.305:  33%|███▎      | 73/220 [00:14<00:30,  4.85it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.305:  34%|███▎      | 74/220 [00:14<00:29,  4.91it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.301:  34%|███▎      | 74/220 [00:15<00:29,  4.91it/s]\u001b[A\u001b[A\n","\n","Training - train_loss: 0.301:  34%|███▍      | 75/220 [00:15<00:29,  4.88it/s]\u001b[A\u001b[A"]},{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-31-9b335f869a8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    237\u001b[0m     hw.run_wizard(func=trainval, exp_list=exp_list, \n\u001b[1;32m    238\u001b[0m                   \u001b[0msavedir_base\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'results/tmp'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m                   reset=True)\n\u001b[0m\u001b[1;32m    240\u001b[0m     \u001b[0;31m# hw.run_wizard(func=trainval, exp_list=exp_list,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0;31m#               savedir='results/tmp',\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/haven/haven_wizard.py\u001b[0m in \u001b[0;36mrun_wizard\u001b[0;34m(func, exp_list, exp_groups, job_config, savedir_base, reset, args, use_threads, exp_id, python_binary_path, python_file_path, workdir, job_scheduler, save_logs, filter_duplicates, results_fname, job_option)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0msavedir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msavedir_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;31m# do trainval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexp_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msavedir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msavedir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-31-9b335f869a8a>\u001b[0m in \u001b[0;36mtrainval\u001b[0;34m(exp_dict, savedir, args)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mscore_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0mtrain_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0;31m# Validate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mval_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_on_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msavedir_images\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msavedir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"images\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_images\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/.shortcut-targets-by-id/1-lqIvGv8GnqHULF2dwocEGdE1OZTOoFv/deepfish/affinity_lcfcn/src/models/semseg.py\u001b[0m in \u001b[0;36mtrain_on_loader\u001b[0;34m(self, train_loader)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mtrain_monitor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainMonitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0mscore_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mtrain_monitor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/.shortcut-targets-by-id/1-lqIvGv8GnqHULF2dwocEGdE1OZTOoFv/deepfish/affinity_lcfcn/src/datasets/sum_fish.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrgb_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatadir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train_val/images\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0mgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrgb_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatadir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train_val/masks\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'val'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrgb_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatadir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TEST/images\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: index 1534 is out of bounds for axis 0 with size 1525"]}],"source":["# Define Training Validation Procedure \n","def trainval(exp_dict, savedir, args):\n","  # def trainval(exp_dict, savedir, datadir, reset=False, num_workers=0):\n","    \"\"\"\n","    exp_dict: dictionary defining the hyperparameters of the experiment\n","    savedir: the directory where the experiment will be saved\n","    args: arguments passed through the command line\n","    \"\"\"\n","        # bookkeepting stuff\n","    # ==================\n","    pprint.pprint(exp_dict)\n","    exp_id = hu.hash_dict(exp_dict) #\n","    os.makedirs(savedir, exist_ok=True)\n","    savedir = os.path.join(savedir, exp_id)\n","    os.makedirs(savedir, exist_ok=True)\n","    hu.save_json(os.path.join(savedir, \"exp_dict.json\"), exp_dict)\n","    print(\"Experiment saved in %s\" % savedir)\n","\n","\n","    # set seed\n","    # ==================\n","    seed = 42\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    print('-----done-torch-cuda-seed-----')\n","    savedir='/content/drive/MyDrive/deepfish/affinity_lcfcn/results'\n","    # ==================\n","    # Dataset\n","    # datadir = 'dataset'\n","    # datadir = '/content/drive/MyDrive/deepfish/Dataset/DeepFish/'\n","    datadir = '/content/drive/MyDrive/deepfish/Dataset/SUIM_seg/'\n","\n","    num_workers=0\n","    # ==================\n","    # train set\n","    train_set = datasets.get_dataset(dataset_dict=exp_dict[\"dataset\"],\n","                                     split=\"train\",\n","                                     datadir=datadir,\n","                                     exp_dict=exp_dict,\n","                                     dataset_size=exp_dict['dataset_size'])\n","\n","    # val set\n","    val_set = datasets.get_dataset(dataset_dict=exp_dict[\"dataset\"],\n","                                   split=\"val\",\n","                                   datadir=datadir,\n","                                   exp_dict=exp_dict,\n","                                   dataset_size=exp_dict['dataset_size'])\n","\n","    # test set\n","    test_set = datasets.get_dataset(dataset_dict=exp_dict[\"dataset\"],\n","                                   split=\"test\",\n","                                   datadir=datadir,\n","                                   exp_dict=exp_dict,\n","                                   dataset_size=exp_dict['dataset_size'])\n","\n","    # # val_sampler = torch.utils.data.SequentialSampler(val_set)\n","    # val_loader = DataLoader(val_set,\n","    #                         # sampler=val_sampler,\n","    #                         batch_size=1,\n","    #                         collate_fn=ut.collate_fn,\n","    #                         num_workers=num_workers)\n","\n","    # test_loader = DataLoader(test_set,\n","    #                         # sampler=val_sampler,\n","    #                         batch_size=1,\n","    #                         collate_fn=ut.collate_fn,\n","    #                         num_workers=num_workers)\n","    # Dataset Loaders\n","    # ==================\n","    train_sampler = torch.utils.data.RandomSampler(\n","                                train_set, replacement=True, \n","                                num_samples=2*len(test_set))\n","\n","    train_loader = DataLoader(train_set,\n","                            sampler=train_sampler,\n","                            collate_fn=ut.collate_fn,\n","                            batch_size=exp_dict[\"batch_size\"], \n","                            drop_last=True, \n","                            num_workers=0)\n","    \n","    val_loader = DataLoader(val_set,\n","                            # sampler=val_sampler,\n","                            batch_size=1,\n","                            collate_fn=ut.collate_fn,\n","                            num_workers=0)\n","    test_loader = DataLoader(test_set,\n","                            # sampler=val_sampler,\n","                            batch_size=1,\n","                            collate_fn=ut.collate_fn,\n","                            num_workers=0)\n","\n","    print('-----done-loader-dts-----')                        \n","\n","    # Model\n","    # ==================\n","    # model = models.get_model(model_dict=exp_dict['model'],\n","    #                          exp_dict=exp_dict,\n","    #                          train_set=train_set, device='cpu')\n","    model = models.get_model(model_dict=exp_dict['model'],\n","                             exp_dict=exp_dict,\n","                             train_set=train_set).cuda()\n","\n","    # model.opt = optimizers.get_optim(exp_dict['opt'], model)\n","    # model_path = os.path.join(savedir, \"model.pth\")\n","    score_list_path = os.path.join(savedir, \"score_list.pkl\")   \n","\n","    # if os.path.exists(score_list_path):\n","    #   # resume experiment\n","    #   model.load_state_dict(hu.torch_load(model_path))\n","    #   score_list = hu.load_pkl(score_list_path)\n","    #   s_epoch = score_list[-1]['epoch'] + 1\n","    # else:\n","    #   # restart experiment\n","    #   score_list = []\n","    #   s_epoch = 0\n","\n","\n","\n","    # Resume or initialize checkpoint\n","    cm = hw.CheckpointManager(savedir, verbose=False)\n","\n","    model_state_dict = cm.load_model()\n","    # if model_state_dict is not None:\n","    #     model.set_state_dict(model_state_dict)\n","        \n","    # Validate the model\n","    # val_dict = model.val_on_loader(val_loader,\n","    #                                savedir_images=os.path.join(savedir, \"images\"),\n","    #                                n_images=3)\n","    val_dict = model.val_on_loader(val_loader,\n","                                   savedir_images=os.path.join(savedir, \"images\"),\n","                                   n_images=60)\n","    # Add main loop\n","    '''\n","    #worked:\n","    for epoch in tqdm.tqdm(range(cm.get_epoch(), 10)):\n","        # Train the model\n","        train_dict = model.train_on_loader(train_loader)\n","\n","        # Validate the model\n","        # val_dict = model.val_on_loader(val_loader,\n","        #                               savedir_images=os.path.join(savedir, \"images\"),\n","        #                               n_images=3)\n","        val_dict = model.val_on_loader(val_loader,\n","                                      savedir_images=os.path.join(savedir, \"images\"),\n","                                      n_images=3)\n","        # Get metrics\n","        score_dict = {'epoch':epoch, \n","                      'val_score': val_dict[\"val_score\"], \n","                      'loss':train_dict['train_loss']}\n","\n","        # Save Metrics and Model\n","        cm.log_metrics(score_dict)\n","        # cm.save_torch(\"model.pth\", model.get_state_dict())\n","\n","        # Report & Save\n","        score_df = pd.DataFrame(score_list)\n","        score_df.to_csv(os.path.join(savedir, \"score_df.csv\"))\n","        print(\"\\n\", score_df.tail(), \"\\n\")\n","        hu.torch_save(model_path, model.get_state_dict())\n","        hu.save_pkl(score_list_path, score_list)\n","        print(\"Checkpoint Saved: %s\" % savedir)\n","\n","    print(\"Experiment completed\")\n","'''\n","# Modified version:\n","    model.waiting = 0\n","    model.val_score_best = -np.inf\n","    score_list = []\n","    s_epoch=0\n","    for e in range(s_epoch, exp_dict['max_epoch']):\n","        # Validate only at the start of each cycle\n","        score_dict = {}\n","        # Train the model\n","        train_dict = model.train_on_loader(train_loader)\n","        # Validate the model\n","        val_dict = model.val_on_loader(val_loader, savedir_images=os.path.join(savedir, \"images\"), n_images=60)\n","        for k in val_dict:\n","            if \"val_\" in k:\n","                score_dict[k] = val_dict[k]\n","\n","        # Get new score_dict\n","        score_dict.update(train_dict)\n","        score_dict[\"epoch\"] = e\n","        score_dict[\"waiting\"] = model.waiting\n","        score_dict.update(val_dict)\n","        model.waiting += 1\n","\n","        # Add to score_list and save checkpoint\n","        score_list += [score_dict]\n","\n","        # Save Best Checkpoint\n","        score_df = pd.DataFrame(score_list)\n","\n","        test_dict = model.val_on_loader(test_loader,\n","                                    savedir_images=os.path.join(savedir, \"images\"),\n","                                    n_images=60)  \n","        score_dict.update(test_dict)\n","\n","        hu.save_pkl(os.path.join(savedir, \"score_list.pkl\"), score_list)\n","\n","        score_df.to_csv(os.path.join(savedir, \"score_df.csv\"))\n","        # hu.torch_save(os.path.join(savedir, \"model.pth\"),model.get_state_dict())\n","\n","        if score_dict[\"val_score\"] >= model.val_score_best:\n","            test_dict = model.val_on_loader(test_loader,\n","                                    savedir_images=os.path.join(savedir, \"images\"),\n","                                    n_images=60)  \n","            score_dict.update(test_dict)\n","\n","            hu.save_pkl(os.path.join(savedir, \"score_list_best.pkl\"), score_list)\n","            score_df.to_csv(os.path.join(savedir, \"score_best_df.csv\"))\n","            # hu.torch_save(os.path.join(savedir, \"model_best.pth\"),model.get_state_dict())\n","            model.waiting = 0\n","            model.val_score_best = score_dict[\"val_score\"]\n","            print(\"Saved Best: %s\" % savedir)\n","\n","        # Report & Save\n","        score_df = pd.DataFrame(score_list)\n","        score_df.to_csv(os.path.join(savedir, \"score_df1.csv\"))\n","        print(\"\\n\", score_df.tail(), \"\\n\")\n","        # hu.torch_save(model_path, model.get_state_dict())\n","        # hu.torch_save(os.path.join(savedir, \"model.pth\"),model.get_state_dict())\n","        hu.save_pkl(score_list_path, score_list)\n","        print(\"Checkpoint Saved: %s\" % savedir)\n","        # Save Metrics and Model\n","        cm.log_metrics(score_dict)\n","        # cm.save_torch(\"model.pth\", model.get_state_dict())\n","    print('Experiment completed et epoch %d' % e)\n","    # hu.torch_save(os.path.join(savedir, \"model.pth\"),model.get_state_dict())\n","\n","\n","\n","if __name__ == '__main__':\n","    # Launch experiments using the trainval function\n","    hw.run_wizard(func=trainval, exp_list=exp_list, \n","                  savedir_base='results/tmp',\n","                  reset=True)\n","    # hw.run_wizard(func=trainval, exp_list=exp_list, \n","    #               savedir='results/tmp',\n","    #               reset=True)    \n","\n","# visualize experiments using dashboard\n","# rm = hr.ResultManager(exp_list=None, savedir_base='results/experiments', \n","#                       job_scheduler=None,\n","#                       verbose=0)\n","\n","\n","hw.run_wizard(func=trainval, exp_list=exp_list,savedir_base='results/tmp',reset=True)\n","rm = hr.ResultManager(exp_list=exp_list, savedir='results/experiments', \n","                      job_scheduler=None,\n","                      verbose=0)\n","# pick specific dataset\n","filterby_list = None\n","legend_list = ['model.name', 'model.loss']\n","title_list = ['dataset.name']\n","y_metrics = ['train_loss', 'test_iou']\n","x_metric = 'epoch'\n","hj.get_dashboard(rm, vars(), wide_display=True, enable_datatables=True)    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Su-N7SB9Lhvq","executionInfo":{"status":"aborted","timestamp":1652343780380,"user_tz":-480,"elapsed":7,"user":{"displayName":"IPSAL Lab-HUST","userId":"14403031417329069761"}}},"outputs":[],"source":["# import pickle\n","\n","\n","# with open('serialized.pkl', 'rb') as f:\n","#     data = pickle.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8x9jRYTCGJ3i","executionInfo":{"status":"aborted","timestamp":1652343780380,"user_tz":-480,"elapsed":6,"user":{"displayName":"IPSAL Lab-HUST","userId":"14403031417329069761"}}},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"iPCO_CaEZItk"},"source":["### Step 5: Visualize the Results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WcUafujg7vlw","executionInfo":{"status":"aborted","timestamp":1652343780380,"user_tz":-480,"elapsed":6,"user":{"displayName":"IPSAL Lab-HUST","userId":"14403031417329069761"}}},"outputs":[],"source":["# # visualize experiments using dashboard\n","# rm = hr.ResultManager(exp_list=None, savedir_base='results/experiments', \n","#                       job_scheduler=None,\n","#                       verbose=0)\n","# # pick specific dataset\n","# filterby_list = None\n","# legend_list = ['model.name', 'model.loss']\n","# title_list = ['dataset.name']\n","# y_metrics = ['train_loss', 'test_iou']\n","# x_metric = 'epoch'\n","# hj.get_dashboard(rm, vars(), wide_display=False, enable_datatables=False)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"run_save_masks_suim.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}